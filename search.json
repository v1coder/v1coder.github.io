[{"title":"Sublime Text 解决 Unable to download XXX 问题","url":"/2018/12/28/Sublime-Text-解决-Unable-to-download-XXX-问题/","content":"\n\n\n#### Sublime Text 安装插件报错：\n\n```\nPackage Control\n\nUnable to download XXX. Please view the console for more details.\n```\n\n<br>\n\n#### 解决方法：\nPreferences  》Package Settings 》 Package Control 》 Settings - User\n\n![image](http://upload-images.jianshu.io/upload_images/9691564-5d69ae02b067077f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n<br>\n\n增加如下内容：\n\n```\n\"debug\": true,\n\"downloader_precedence\":\n{\n\t\"linux\": [ \"curl\", \"urllib\", \"wget\" ],\n\t\"osx\": [ \"curl\", \"urllib\" ],\n\t\"windows\": [ \"wininet\" ]\n},\n```\n\n\n\n最终内容如下：\n\n```\n{\n\t\"bootstrapped\": true,\n\t\"debug\": true,\n\t\"installed_packages\":\n\t[\n\t\t\"Package Control\"\n\t],\n\t\"downloader_precedence\":\n\t{\n\t\t\"linux\": [ \"curl\", \"urllib\",    \"wget\" ],\n\t\t\"osx\": [ \"curl\", \"urllib\" ],\n\t\t\"windows\": [ \"wininet\" ]\n\t},\n}\n```\n\n再次安装插件就没问题了。\n\n------\n\n鸣谢：\n[Sublime安装及解决There are no packages available for installation](https://www.jianshu.com/p/d1d97f8ac64b)\n[Mac sublime 安装包的时候出现 unable to download xxx](https://blog.csdn.net/csdn_yudong/article/details/84866428)\n[CERTIFICATE_VERIFY_FAILED on macOS Sierra Beta](https://github.com/wbond/package_control/issues/1220)","tags":["sublime text"]},{"title":"Python 的 Beautiful Soup 库","url":"/2018/12/27/Python-的-Beautiful-Soup-库/","content":"\n\n\nBeautiful Soup 4已经被移植到BS4了，所以要 \n\n```\nfrom bs4 import BeautifulSoup \n```\n\n\n\n创建 beautifulsoup 对象 \n\n```\nsoup = BeautifulSoup(html, 'lxml') \n```\n\n另外，我们还可以用本地 HTML 文件来创建对象，例如 \n\n```\nsoup = BeautifulSoup(open('index.html'), 'lxml') \n```\n\n\n\n格式化输出： \n\n```\nprint soup.prettify() \n```\n\n<br>\n\n------\n\n```\nhtml_doc = \"\"\"\n<html><head><title>The Dormouse's story</title></head>\n<body>\n<p class=\"title\"><b>The Dormouse's story</b></p>\n\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\nand they lived at the bottom of a well.</p>\n\n<p class=\"story\">...</p>\n\"\"\"\n```\n\n上面是示例文档，后面演示的都是搜索上面 `html_doc` 中的内容\n\n```\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html_doc)\n```\n\n\n\n------\n\n### BeautifulSoup 过滤器\n\n这些过滤器可以被用在 tag 的 name 中，节点的属性中，字符串中或他们的混合中。 \n\n\n1.字符串 \n传入字符串，查找与字符串完整匹配的内容 \n\n\n\n2.正则表达式 \n传入正则表达式 re.compile(规则)，通过正则表达式来匹配内容 \n\n\n\n3.列表 \n传入列表参数，如 ['div','a','b']，返回与列表中任一元素匹配的内容。 \n\n\n\n4.True \n匹配任何值 \n\n\n\n5.方法 \n还可以传入自定义函数 \n\n<br>\n\n------\n\n\n\n### find_all 方法\n\n搜索当前 tag 的所有子节点,并判断是否符合过滤器的条件。\n返回的结果是所有符合条件的 tag 组成的列表。\n\n语法： \n\n```\nfind_all( name , attrs , recursive , string , **kwargs )\n```\n\n<br>\n\n#### name 参数\n\n查找所有名字为 name 的 tag \nname 参数的值可以使任一类型的 过滤器：字符串，正则表达式，列表，方法或是 True . \n\n\n\na. 给 name 参数传入字符串 \n\n```\nsoup.find_all(\"title\")\n# [<title>The Dormouse's story</title>]\n\nsoup.find_all('b')\n# [<b>The Dormouse's story</b>]\n\nprint soup.find_all('a')\n#[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n```\n\n\n\nb.传入正则表达式 \n\n```\n# 找出所有以b开头的标签,这表示<body>和<b>标签都应该被找到\nimport re\nfor tag in soup.find_all(re.compile(\"^b\")):\n    print(tag.name)\n# body\n# b\n\n# 找出所有名字中包含”t”的标签:\nfor tag in soup.find_all(re.compile(\"t\")):\n    print(tag.name)\n# html\n# title\n```\n\n\n\nc.传入列表 \n\n```\n# 找到文档中所有<a>标签和<b>标签:\nsoup.find_all([\"a\", \"b\"])\n# [<b>The Dormouse's story</b>,\n#  <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n```\n\n\n\nd.传入True \nTrue 可以匹配任何值,下面代码查找到所有的tag \n\n```\nfor tag in soup.find_all(True):\n    print(tag.name)\n# html\n# head\n# title\n# body\n# p\n# b\n# p\n# a\n# a\n# a\n# p\n```\n\n<br>\n\n#### keyword 参数\n\n如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字tag的属性来搜索 \n参数类型包括：字符串 , 正则表达式 , 列表, True . \n\n\n\n1.传入字符串 \n\n```\n# 找到所有属性名为 id 且属性值为 link2 的字符串\nsoup.find_all(id='link2')\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n```\n\n\n\n2.传入正则表达式 \n\n```\nsoup.find_all(href=re.compile(\"elsie\"))\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n```\n\n\n\n3.True \n\n```\n# 查找所有包含 id 属性的tag,无论 id 的值是什么:\nsoup.find_all(id=True)\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n```\n\n\n\n同时匹配多个属性 \n\n```\nsoup.find_all(href=re.compile(\"elsie\"), id='link1')\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">three</a>]\n```\n\n\n\n想搜索 class 属性，但 class 是 python 关键字，所以用 `class_` 代替 \n\n```\nsoup.find_all(\"a\", class_=\"sister\")\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n```\n\n\n\nHTML5中的 data-* 属性在搜索不能使用 \n\n```\ndata_soup = BeautifulSoup('<div data-foo=\"value\">foo!</div>')\ndata_soup.find_all(data-foo=\"value\")\n# SyntaxError: keyword can't be an expression\n```\n\n可以通过 find_all() 方法的 attrs 参数定义一个字典参数来搜索包含特殊属性的tag \n\n```\ndata_soup.find_all(attrs={\"data-foo\": \"value\"})\n# [<div data-foo=\"value\">foo!</div>]\n```\n\n<br>\n\n\n\n#### string 参数\n\n匹配 文档 中的字符串内容，返回字符串列表 \nstring 参数接受 字符串 , 正则表达式 , 列表, True 和方法 \n\n```\nsoup.find_all(text=\"Elsie\")\n# [u'Elsie']\n\nsoup.find_all(text=[\"Tillie\", \"Elsie\", \"Lacie\"])\n# [u'Elsie', u'Lacie', u'Tillie']\n\nsoup.find_all(text=re.compile(\"Dormouse\"))\n[u\"The Dormouse's story\", u\"The Dormouse's story\"]\n```\n\n\n\n可以与其它参数混合使用 \n\n```\n# 搜索内容里面包含“Elsie”的<a>标签:\nsoup.find_all(\"a\", string=\"Elsie\")\n# [<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>]\n```\n\n<br>\n\n\n\n#### limit 参数\n\n设置匹配上限，当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果 \n\n```\n# 文档树中有3个tag符合搜索条件,但结果只返回2个\nsoup.find_all(\"a\", limit=2)\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n```\n\n<br>\n\n\n\n#### recursive 参数\n\nfind_all() 方法时默认检索当前 tag 的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False \n\n一段简单的文档:\n\n```\n<html>\n<head>\n  <title>\n   The Dormouse's story\n  </title>\n</head>\n...\n```\n\n是否使用 recursive 参数的搜索结果: \n\n```\nsoup.html.find_all(\"title\")\n# [<title>The Dormouse's story</title>]\n \nsoup.html.find_all(\"title\", recursive=False)\n# []\n```\n<title>标签在 <html> 标签下, 但并不是直接子节点, <head> 标签才是直接子节点. 在允许查询所有后代节点时能够查找到 <title> 标签. 但是使用了 recursive=False 参数之后,只能查找直接子节点,这样就查不到 <title> 标签了.\n\n<br>\n\nfind_all() 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. \n\n下面两行代码是等价的: \n\n```\nsoup.find_all(\"a\")\nsoup(\"a\")\n```\n\n\n\n这两行代码也是等价的: \n\n```\nsoup.title.find_all(string=True)\nsoup.title(string=True)\n```\n\n<br>\n\n------\n\n\n\n### find() 方法\n\n```\nfind( name , attrs , recursive , string , **kwargs )\n```\n\n\n\nfind() 与 find_all() 区别： \n\nfind_all() 返回符合条件的所有 tag，find() 只返回符合条件的第一个 tag \n\nfind_all() 返回结果是列表,而 find() 方法直接返回结果. \n\nfind_all() 方法没有找到目标时返回空列表, find() 方法找不到目标时,返回 None . \n\n\n\n```\nsoup.find_all('title', limit=1)\n# [<title>The Dormouse's story</title>]\n\nsoup.find('title')\n# <title>The Dormouse's story</title>\n```\n\n<br>\n\n------\n\n\n\n#### 标签的属性 attrs\n\n\n\n把标签的所有属性打印输出了出来，结果为字典类型 \n\n```\nprint soup.p.attrs\n#{'class': ['title'], 'name': 'dromouse'}\n```\n\n\n\n单独获取某个属性的值 \n\n```\nprint soup.p['class']\n#['title’]\n\nprint soup.p.get('class')\n#['title']\n```\n\n<br>\n\n------\n\n\n\n### select() 方法\n\n用 [CSS 选择器](http://www.w3school.com.cn/cssref/css_selectors.ASP) 的语法来筛选元素，返回 tag 列表 \n\nCSS选择器语法： \n标签名不加任何修饰，类名（class=\"className\"引号内即为类名）前加点，id名id=\"idName”引号内即为id名）前加 # \n\n<br>\n\n#### 通过 tag 名查找\n\n```\nprint soup.select('title') \n#[<title>The Dormouse's story</title>]\n\nprint soup.select('a')\n#[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n```\n\n<br>\n\n#### 通过类名查找\n\n```\nprint soup.select('.sister')\n#[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n```\n\n<br>\n\n#### 通过 id 名查找\n\n```\nprint soup.select('#link1')\n#[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n```\n\n<br>\n\n#### 通过属性查找\n\n查找时还可以加入属性元素，属性需要用中括号括起来\n\n```\nprint soup.select('a[class=\"sister\"]')\n#[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n\nprint soup.select('a[href=\"http://example.com/elsie\"]')\n#[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n```\n\n<br>\n\n是否存在某个属性来查找: \n\n```\nsoup.select('a[href]')\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n```\n\n<br>\n\n多个查找条件属于同一 tag 的，不用空格隔开； \n\n多个查找条件不属于同一 tag 的，用空格隔开。 \n\n<br>\n\n（同时符合条件1和条件2的 tag） \n选择标签名为 a，id 为 link2 的 tag： \n\n```\nsoup.select('a#link2’)\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n```\n\n<br>\n\ntag 之间的包含查找 \n查找标签 p 中，id 等于 link1 的 tag，二者需要用空格分开 \n\n```\nprint soup.select('p #link1')\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n```\n\n<br>\n\n找到某个tag标签下的直接子标签 \n\n```\nsoup.select(\"head > title\")\n# [<title>The Dormouse's story</title>]\n\nsoup.select(\"p > a\")\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n#  <a class=\"sister\" href=\"http://example.com/lacie\"  id=\"link2\">Lacie</a>,\n#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n\nsoup.select(\"p > #link1\")\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n\nsoup.select(\"body > a\")\n# []\n```\n\n<br>\n\n同时用多种CSS选择器查询（符合条件1或条件2的tag）: \n\n```\nsoup.select(\"#link1,#link2\")\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n```\n\n<br>\n\n------\n\n\n\n用 beautifulsoup 获取 HTML 网页源码里的内容，想删除或替换里面的 &nbsp; \n\n使用 \\xa0 \n\n```\n>>> soup = BeautifulSoup('<div>a&nbsp;b</div>', 'lxml')\n>>> soup.prettify()\nu'<html>\\n <body>\\n  <div>\\n   a\\xa0b\\n  </div>\\n </body>\\n</html>'\n```\n\n<br>\n\n------\n\n\n\n#### .text与.string\n\n\n\n在用find()方法找到特定的tag后，想获取里面的文本，可以用.text属性或者.string属性。 \n\n在很多时候，两者的返回结果一致，但其实两者是有区别的 \n\n\n\n例如 html 像这样： \n\n```\n1、<td>some text</td> \n2、<td></td>\n3、<td><p>more text</p></td>\n4、<td>even <p>more text</p></td>\n```\n\n\n\n.string 属性得到的结果 \n\n```\n1、some text\n2、None\n3、more text\n4、None\n```\n\n\n\n.text 属性得到的结果 \n\n```\n1、some text\n\n2、more text\n3、even more text\n```\n\n\n\n.find和.string之间的差异： \n\n第一行，td没有子标签，且有文本时，两者的返回结果一致，都是文本 \n\n第二行，td没有子标签，且没有文本时，.string返回None，.text返回为空 \n\n第三行，td只有一个子标签时，且文本只出现在子标签之间时，两者返回结果一致，都返回子标签内的文本 \n\n第四行，最关键的区别，td有子标签，并且父标签td和子标签p各自包含一段文本时，两者的返回结果，存在很大的差异： \n\n.string返回为空，因为文本数>=2，string不知道获取哪一个 \n\n.text返回的是，两段文本的拼接。 \n\n<br>\n\n------\n\n\n\n### 使用 BeautifulSoup 提取网页内容 demo\n\n```\n# python3\n# -*- coding: utf-8 -*-\n# Filename: BeautifulSoup_demo.py\n\"\"\"\n练习使用 BeautifulSoup 提取网页内容\n\n@author: v1coder\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nheaders = {'User-Agent':''}\n\n\n# 爬取biqukan.com 的小说《一念永恒》\ndef biqukan_com():\n    url = 'https://www.biqukan.com/1_1094/5403177.html'\n    req = requests.get(url)\n#    text_data = req.text 也可以\n    content_data = req.content\n    soup = BeautifulSoup(content_data, 'lxml')\n#    print(soup.prettify())\n    texts = soup.find_all('div', id=\"content\")  \n#    texts = soup.find_all('div', class_=\"showtxt\") #也可以\n    print(texts[0].text.replace('\\xa0'*8,'\\n'))\n#biqukan_com()\n\n# 爬取吐槽大会第三季每期标题\ndef tucaodahui_title():\n\turl = 'https://v.qq.com/detail/8/80844.html'\n\tdata = requests.get(url).text\n\tsoup = BeautifulSoup(data, 'lxml')\n\ttitles = soup.find_all('strong', itemprop=\"episodeNumber\")\n\tfor title in titles:\n\t\tprint(title.text)\n#tucaodahui_title()\n\n# 爬取简书纯文本文章\ndef jianshu():\n\turl = 'https://www.jianshu.com/p/713415f82576'\n\tdata = requests.get(url, headers=headers).text\n\tsoup = BeautifulSoup(data, 'lxml')\n\ttexts = soup.find_all('div', class_=\"show-content-free\")\n\tprint(texts[0].text)\n#jianshu()\n\n# 豆瓣电影 TOP250\ndef douban_TOP250():\n\turl = 'https://movie.douban.com/top250'\n\tdata = requests.get(url, headers=headers).text\n\tsoup = BeautifulSoup(data, 'lxml')\n\tcomments = soup.find_all('span', class_=\"inq\")\n\ttitles = soup.find_all('img', width=\"100\")\n\tfor num in range(len(titles)):\n\t\t# titles[num].get('alt') get方法，传入属性的名称，获得属性值\n\t\ttitle = str(num+1) + '.' + '《' + titles[num].get('alt') + '》'\n\t\tcomment = '  ：' + comments[num].text\n\t\tprint(title)\n\t\tprint(comment)\n\t\tprint()\n#douban_TOP250()\n\n# 电影天堂最新电影\ndef dytt():\n\turl = 'https://www.dytt8.net/'\n\tdata = requests.get(url, headers=headers).content\n\tsoup = BeautifulSoup(data, 'lxml')\n\ttext = soup.find('div', class_=\"co_content8\")\n\tdates = text.find_all('font')  # 得到日期\n#\tfor date in dates[1:]:\n#\t\tprint(date.text)\n\tnames = text.select(\"td a\")  # 得到电影名\n\tnum = 1\n\tfor name in names[2::2]:\n\t\tprint(name.text)\n\t\tprint(dates[num].text)\n\t\tprint()\n\t\tnum += 1\n#dytt()\n\n# 妹子图1，输出图片链接\ndef mmjpg():\n\turl = 'http://www.mmjpg.com/'\n\tdata = requests.get(url, headers=headers).content\n\tsoup = BeautifulSoup(data, 'lxml')\n\turl_tags = soup.find_all('img', width=\"220\")\n\tfor url_tag in url_tags:\n\t\tpic_url = url_tag['src']  # 获得属性值用 ['src'] 或 get('src')\n\t\tprint(pic_url)\n#mmjpg()\n\n# 妹子图2，输出图片链接\ndef haopic_me():\n\turl = 'http://www.haopic.me/tag/meizitu'\n\tdata = requests.get(url, headers=headers).content\n\tsoup = BeautifulSoup(data, 'lxml')\n\turl_tags = soup.find_all('div', class_=\"post\")\n\tfor url_tag in url_tags:\n\t\tpic_url = url_tag.find('img')['src']  \n\t\tprint(pic_url)\n#haopic_me()\n\n# 妹子图3，输出图片链接\ndef mzitu_com():\n\turl = 'https://www.mzitu.com/'\n\tdata = requests.get(url, headers=headers).content\n\tsoup = BeautifulSoup(data, 'lxml')\n\turl_tags = soup.find_all('img', class_='lazy')\n\tfor url_tag in url_tags:\n\t\tpic_url = url_tag.get('data-original')\n\t\tprint(pic_url)\n#mzitu_com()\n\n```\n\n<br>\n\n------\n\n\n\n鸣谢：\n[Beautiful Soup的用法 | 静觅](http://cuiqingcai.com/1319.html)\n[Beautiful Soup 4.4.0 文档](http://beautifulsoup.readthedocs.io/zh_CN/latest/#id8)\n[BeautifulSoup解析网页](https://segmentfault.com/a/1190000014512935)\n\n2018-12-27","tags":["spider","python","beautifulsoup"]},{"title":"煎蛋妹子图爬虫","url":"/2018/12/25/煎蛋妹子图爬虫/","content":"\n\n\n![](http://wx1.sinaimg.cn/mw600/82e98952ly1fygoarfjxqj20uo1hc7ci.jpg)\n\n为了不增加网站的负担，这里只做一个demo，输出一页的图片地址\n<br>\n\n---\n\n### 代码：\n\n```python\n# python3\n# -*- coding: utf-8 -*-\n# Filename: jandan_demo.py\n\"\"\"\n输出煎蛋妹子图地址\n\n@author: v1coder\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport base64\n\nheaders = {'User-Agent': ''}\n\nurl = 'http://jandan.net/top-ooxx'\ndata = requests.get(url, headers=headers).content\nsoup = BeautifulSoup(data, 'lxml')\nurl_tags = soup.find_all('span', class_=\"img-hash\")\nfor url_tag in url_tags:\n    url_hash = url_tag.text\n    # base64 to bytes\n    url_bytes = base64.b64decode(url_hash)\n    # bytes to str\n    url = str(url_bytes, encoding='utf-8')\n    complete_url = 'http:' + url\n    print(complete_url)\n```\n<br>\n\n---\n\n煎蛋的网页源代码找不到图片地址，所以我们换一种方法\n<br>\n### 具体方法：\n\n- 找到图片代码位置\n\n打开 Chrome 浏览器开发者工具（Mac 下快捷键 option+command+I），点击开发者工具左上角的箭头，再将点击网页中的图片，就能定位到图片代码的位置\n\n![](https://blog-pic-1253208066.file.myqcloud.com/2018-12-24-144607.png)\n\n<br>\n\n- 网页源码中找信息\n\nChrome 浏览器打开网页源码（Mac 下快捷键 option+command+U），在本该出现图片地址的位置是以下代码\n\n```html\n<p><img src=\"//img.jandan.net/img/blank.gif\" onload=\"jandan_load_img(this)\" /><span class=\"img-hash\">Ly93eDEuc2luYWltZy5jbi9tdzYwMC84MmU5ODk1Mmx5MWZ5Z29hcmZqeHFqMjB1bzFoYzdjaS5qcGc=</span></p>\n```\n\n这里有两个关键信息：\n\n`jandan_load_img(this)` 函数和`\"img-hash\">` 后面的值\n\n<br>\n\n- 解码网址\n\n先从 `jandan_load_img(this)` 入手\n\n开发者模式下全局搜索（Windows快捷键 ctrl+shift+f，Mac 快捷键 option+command+F），搜函数名`jandan_load_img` ，找到对应的的 js 文件，再点击花括号\n\n![](https://blog-pic-1253208066.file.myqcloud.com/2018-12-24-151152.png)\n\n<br>\n\n就可以看到 js 文件的代码，在里面搜索（快捷键 command+F）函数名`jandan_load_img`\n\n```javascript\nfunction jandan_load_img(b) {\n    var d = $(b);\n    var f = d.next(\"span.img-hash\");\n    var e = f.text();\n    f.remove();\n    var c = jdQFXcEeWzpGANTZyHb1G0w0ggDlCZ5ILV(e, \"qOzLfOL8mfbbsawjoQQPkWwkakHnOGze\");\n    var a = $('<a href=\"' + c.replace(/(\\/\\/\\w+\\.sinaimg\\.cn\\/)(\\w+)(\\/.+\\.(gif|jpg|jpeg))/, \"$1large$3\") + '\" target=\"_blank\" class=\"view_img_link\">[查看原图]</a>');\n    d.before(a);\n    d.before(\"<br>\");\n    d.removeAttr(\"onload\");\n    d.attr(\"src\", location.protocol + c.replace(/(\\/\\/\\w+\\.sinaimg\\.cn\\/)(\\w+)(\\/.+\\.gif)/, \"$1thumb180$3\"));\n    if (/\\.gif$/.test(c)) {\n        d.attr(\"org_src\", location.protocol + c);\n        b.onload = function() {\n            add_img_loading_mask(this, load_sina_gif)\n        }\n    }\n}\n```\n\n大概意思是说取到 `class='img-hash'` 的 span 标签中的值, 通过 `jdQFXcEeWzpGANTZyHb1G0w0ggDlCZ5ILV` 这个函数处理之后, 然后再拼接成图片的地址。\n\n<br>\n\n 我们再查找` jdQFXcEeWzpGANTZyHb1G0w0ggDlCZ5ILV` 这个函数\n\n```javascript\nvar jdQFXcEeWzpGANTZyHb1G0w0ggDlCZ5ILV = function(o, y, g) {\n    var d = o;\n    var l = \"DECODE\";\n    var y = y ? y : \"\";\n    var g = g ? g : 0;\n    var h = 4;\n    y = md5(y);\n    var x = md5(y.substr(0, 16));\n    var v = md5(y.substr(16, 16));\n    if (h) {\n        if (l == \"DECODE\") {\n            var b = md5(microtime());\n            var e = b.length - h;\n            var u = b.substr(e, h)\n        }\n    } else {\n        var u = \"\"\n    }\n    var t = x + md5(x + u);\n    var n;\n    if (l == \"DECODE\") {\n        g = g ? g + time() : 0;\n        tmpstr = g.toString();\n        if (tmpstr.length >= 10) {\n            o = tmpstr.substr(0, 10) + md5(o + v).substr(0, 16) + o\n        } else {\n            var f = 10 - tmpstr.length;\n            for (var q = 0; q < f; q++) {\n                tmpstr = \"0\" + tmpstr\n            }\n            o = tmpstr + md5(o + v).substr(0, 16) + o\n        }\n        n = o\n    }\n    var k = new Array(256);\n    for (var q = 0; q < 256; q++) {\n        k[q] = q\n    }\n    var r = new Array();\n    for (var q = 0; q < 256; q++) {\n        r[q] = t.charCodeAt(q % t.length)\n    }\n    for (var p = q = 0; q < 256; q++) {\n        p = (p + k[q] + r[q]) % 256;\n        tmp = k[q];\n        k[q] = k[p];\n        k[p] = tmp\n    }\n    var m = \"\";\n    n = n.split(\"\");\n    for (var w = p = q = 0; q < n.length; q++) {\n        w = (w + 1) % 256;\n        p = (p + k[w]) % 256;\n        tmp = k[w];\n        k[w] = k[p];\n        k[p] = tmp;\n        m += chr(ord(n[q]) ^ (k[(k[w] + k[p]) % 256]))\n    }\n    if (l == \"DECODE\") {\n        m = base64_encode(m);\n        var c = new RegExp(\"=\",\"g\");\n        m = m.replace(c, \"\");\n        m = u + m;\n        m = base64_decode(d)\n    }\n    return m\n};\n```\n\n大概意思是对图片网址进行了 base64 编码，所以我们尝试对源码中的`\"img-hash\">` 后面的值进行 base64 解码。\n\n解码方式：\n\n```python\nimport base64\nb = 'Ly93eDEuc2luYWltZy5jbi9tdzYwMC84MmU5ODk1Mmx5MWZ5Z29hcmZqeHFqMjB1bzFoYzdjaS5qcGc='\nbase64.b64decode(b)\n# 输出\nb'//wx1.sinaimg.cn/mw600/82e98952ly1fygoarfjxqj20uo1hc7ci.jpg'\n```\n\n<br>\n\n- 最后\n\n把 bytes 对象转换成 str ，再把网址拼接完整就可以了。\n\n```python\nurl = str(url_bytes, encoding='utf-8')\ncomplete_url = 'http:' + url\n```\n\n---\n鸣谢：\n[hellospider - jandan](https://github.com/hellospider/jandan)\n[Python爬虫爬取煎蛋网无聊图](https://www.jianshu.com/p/5351baf254ef)\n[oversplit - Jandan](https://github.com/oversplit/Jandan/blob/master/meizi.py)\n\n2018-12-25","tags":["spider","python"]},{"title":"配置 hexo 的 aircloud 主题","url":"/2018/12/14/配置-hexo-的-aircloud-主题/","content":"\n\n\n<br>\n\n### 网站图标\n\n![](https://upload-images.jianshu.io/upload_images/9691564-08239664a7bdcc22.png)\n\n在你的博客根目录的 `/source/img/` 下面添加 favicon.ico 即可，不是在主题文件夹内添加。\n\n<br>\n\n### 语言\n\n修改博客根目录下 `_config.yml` 文件，`language: zh` 为中文，`language:  en` 为英文。（注意冒号后面的空格，下同） \n\n<br>\n\n### 博客作者\n\n![](https://upload-images.jianshu.io/upload_images/9691564-75b8756a9c7c3ac0.png)\n\n修改博客根目录下 `_config.yml` 文件的 `author:`\n\n<br>\n\n### 标签页面和关于页面\n\n如果是新项目，默认是没有`标签`页面和`关于`页面的，需要在博客根目录的`source`文件夹下建立`tags`文件夹和`about`文件夹。\n\n> 注：建议不要直接新建文件，而是采用 hexo 的 `hexo new page tags` 和 `hexo new page about` 的方式新建文件，这样可以被 hexo 索引到。\n\n其中`tags`文件夹中新建`index.md`并写入：\n\n```\n---\nlayout: \"tags\"\ntitle: \"Tags\"\n---\n```\n\n\n\n`about`文件夹下`index.md`为一篇支持 markdown 格式的文件，需要在开头添加：\n\n```\n---\nlayout: \"about\"\ntitle: \"About\"\ndate: 2016-04-21 04:48:33\ncomments: true\n---\n```\n\n<br>\n\n### 头像\n\n![](https://upload-images.jianshu.io/upload_images/9691564-7027ab6325ec6f07.png)\n\n在博客根目录的 source 文件夹下建立 img 文件夹，并将头像文件命名为avatar.jpg\n\n编辑博客根目录的 _config.yml 文件，增加：\n\n```\nsidebar-avatar: img/avatar.jpg\n```\n\n<br>\n\n### 网站标题\n\n![](https://ws1.sinaimg.cn/large/006tNbRwly1fy4eiqsnb4j30j7022aa1.jpg)\n\n博客根目录 _config.yml 文件，增加\n\n```\nSEOTitle: [网站标题]\n```\n\n<br>\n\n### 副标题\n\n编辑根目录的 _config.yml 文件\n\n```\nsubtitle: Stay Hungry，Stay Foolish\n```\n\n效果：\n\n![](https://ws3.sinaimg.cn/large/006tNbRwly1fy4efh7bv3j30tp08mjs1.jpg)\n\n<br>\n\n### 增加搜索功能\n\n安装插件：\n\n```\nnpm i hexo-generator-search --save\n```\n\n编辑根目录的 _config.yml 文件，增加：\n\n```\nsearch:\n  path: search.json\n  field: post\n```\n\n<br>\n\n### 社交网络链接\n\n编辑根目录的 _config.yml 文件，增加：\n\n```\n# 社交平台的样例代码：\nweibo_username:     3286578617\nzhihu_username:     ai-er-lan-xue-da\ngithub_username:    AirCloud\ntwitter_username:   iconie_alloy\nfacebook_username:  xiaotao.nie.5\nlinkedin_username:  小涛-聂-80964aba\n```\n\n<br>\n\n### 添加赞赏\n\n把收款二维码放到根目录 /source/img/ 文件夹下，重命名为 donate.jpg\n\n在根目录的 _config.yml 文件，增加：\n\n```\ndonate:\n  img: img/donate.jpg\n  content: 感谢鼓励\n```\n\n如此，每篇文章的结尾就会出现赞赏\n\n<br>\n\n### 首行缩进\n\n默认是有首行缩进两个汉字的效果，但是也可以通过下面的配置进行关闭：\n\n在根目录的 _config.yml 文件，增加\n\n```\npost_style:\n    indent: 0\n```\n\n<br>\n\n### 代码块\n\n编辑根目录的 _config.yml 文件：\n\n```\nhighlight: #代码块的设置\n  enable: true\n  line_number: true #是否显示行号\n  auto_detect: true #代码自动高亮\n  tab_replace:\n```\n\n<br>\n\n\n\n------\n\n鸣谢：\n[官方教程](https://github.com/aircloud/hexo-theme-aircloud)\n[aircloud 安装笔记](https://www.bubuyu.work/2018/08/06/Hexo-AirCloud%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/)\n\n","tags":["blog","hexo"]},{"title":"修改 Mac 截图默认文件名","url":"/2018/12/13/修改-Mac-截图默认文件名/","content":"\n<br>\n\nMac 截图：\n\n`Cmd+Shift+3`：全屏截图\n\n`Cmd+Shift+4`：区域截图\n\n`Cmd+Shift+4 + 单击空格键`：窗口截图\n\n<br>\n\n### 修改截图默认文件名\n\n默认截图文件名由前缀和时间戳两部分组成\n\n```\n屏幕快照 2018-12-13 下午7.03.56\n```\n\n- 修改前缀\n\n终端输入\n\n```\ndefaults write com.apple.screencapture name Screenshot\n```\n\nScreenshot 改成你喜欢的前缀。如果有特殊字符（例如空格），需要包含在引号内 `\"截 图\"`\n\n终端再输入\n\n```\nkillall SystemUIServer\n```\n\n\n\n- 移除时间戳\n\n终端输入\n\n```\ndefaults write com.apple.screencapture \"include-date\" 0\nkillall SystemUIServer\n```\n\n（如果想恢复时间戳，将「1」替换成「0」即可。）","tags":["mac"]},{"title":"GitHub page 独立域名解析与域名邮箱解析冲突","url":"/2018/12/12/GitHub-page-独立域名解析与域名邮箱解析冲突/","content":"\n<br>\n\nGitHub page 独立域名的解析 CNAME 记录，与域名邮箱的解析 MX 记录冲突\n\n解决方法：将 `github pages` 的 CNAME 记录更换为 A 记录，使用 ip 作为记录值\n\n<br>\n\n具体：\n\n- 先把原来的 GitHub page 解析删除，\n\n\n\n- 添加新的 GitHub page 解析\n\n```\n@          A             185.199.108.153\n@          A             185.199.109.153\nwww      CNAME           username.github.io\n```\n\n\n\n- 然后就可以添加域名邮箱解析了。\n\n<br>\n\n也可以使用类似 blog.yourdomain 这样的二级域名作为博客域名，避免对根域名使用 CNAME 记录。","tags":["blog"]},{"title":"Hexo 更换主题","url":"/2018/12/11/Hexo-更换主题/","content":"\n\n\n<br>\n\n更换 Hexo 主题非常容易，只要在 `themes` 文件夹内，新增一个任意名称的文件夹，并修改 `_config.yml` 内的 `theme` 设定，即可切换主题。\n\n<br>\n\n具体步骤：\n\n### 1.安装主题\n\n到主题的 GitHub 主页下，复制地址\n\n![](https://upload-images.jianshu.io/upload_images/9691564-d0ac39a0f71d93aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n在终端 cd 到博客根目录\n\n输入命令： `git clone 刚才复制的地址 themes/xxxx` ，`xxxx` 是主题名\n\n例如：\n\n```\ngit clone https://github.com/theme-next/hexo-theme-next.git themes/next\n```\n\n或者：\n\n从 https://hexo.io/themes/ 下载 release 包，解压到博客根目录 `themes/` 文件夹下\n\n<br>\n\n### 2.使用主题\n\n修改博客根目录下的 `_config.yml` 文件，注释掉原来的 theme 并新增一句 `theme: xxxx` （注意冒号后面有空格）\n\n![](https://upload-images.jianshu.io/upload_images/9691564-6dd3351293d20d11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nxxxx 为博客根目录 themes/ 文件夹下，对应的主题文件夹名称。\n\n如果是解压出来的“hexo-theme-aircloud”就写“hexo-theme-aircloud”，如果是自建的“aircloud”就写“aircloud”\n\n<br>\n\n### 3.\n\n本地预览\n\n```\nhexo clean\nhexo g\nhexo s\n```\n\n打开 http://localhost:4000/ 预览效果\n\n<br>\n\n部署博客\n\n```\nhexo clean\nhexo g\nhexo d\n```\n\n<br>\n\n","tags":["blog","hexo"]},{"title":"Mac 终端设置命令别名","url":"/2018/12/11/Mac-终端设置命令别名/","content":"\n\n\n<br>\n\n```\n# 进入根目录\ncd ~  \n\n# 创建bash文件\ntouch .bash_profile\n\n# 编辑\nvim .bash_profile\n\n# 使文件生效\nsource .bash_profile\n```\n\n<br>\n\n```\n# 查看现有别名\nalias\n\n# 创建命令别名之前先测试命令名是否已存在\n# 例如测试 foo\ntype foo \nbash: type: foo: not found\n# 输出 not found 说明未存在\n```\n\n<br>\n\n为多个命令的组合设置别名：\n\n```\nalias hcgd='hexo clean&&hexo g&&hexo d'\n```\n\n（如果不管命令执行是否成功都往下执行，可以用 `;` 分割多个语句放在一行执行，如果希望前面的执行成功才执行后面的语句，用 `&&` 分割多个语句放在一行执行。）","tags":["mac","linux"]},{"title":"使用 Hexo 在 Github 上建博客","url":"/2018/12/11/使用-Hexo-在-Github-上建博客/","content":"\n\n\n先确认  git 与 npm 已经安装，在终端输入以下命令\n\n```\ngit --version\nnpm --version\n```\n\n<br>\n\n### 安装 hexo，在终端输入\n\n```\nnpm install hexo-cli -g\n```\n\n安装过程中报错\n\n![](https://upload-images.jianshu.io/upload_images/9691564-709e80ccccc33db3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n解决方法：\n\n```\nsudo npm install hexo-cli -g\n```\n\n确认 hexo 安装成功：\n\n```\nhexo version\n```\n\n<br>\n\n来到你中意的目录（文件夹）：\n\n```\ncd /Users/v1coder/Documents/\n```\n\n然后\n\n```\nhexo init blog\ncd blog\nnpm install\nhexo g\nhexo server\n```\n\n此时新建了一个 blog 文件夹，并且 blog 里创建了所需要的文件。（ blog 可以改成你中意的其他文件夹名）\n\n这些完成之，你就可以打开浏览器，在地址栏里输入： `http://localhost:4000`，在本地先看看网站是什么样子。\n\n<br>\n\n### 配置秘钥\n\n- 检查 SSH keys 是否已经存在\n\n终端执行如下命令：\n\n```\nls ~/.ssh\n```\n\n如果显示如下信息(重点是要有`id_rsa`和`id_rsa.pub`)，就说明 SSH keys 已经存在了：\n\n```\nid_rsa\t   id_rsa.pub\t  known_hosts\n```\n\n如果存在，则直接跳到 `将 SSH key 添加到 GitHub 中`\n\n<br>\n\n- 生成新的Key：（引号内的内容替换为你自己的邮箱）\n\n\n```\nssh-keygen -t rsa -C \"your_email@youremail.com\"\n```\n\n 输出显示：\n\n```\n>Generating public/private rsa key pair. Enter file in which to save the key \n(/Users/your_user_directory/.ssh/id_rsa):<press enter>\n```\n\n  直接回车，不要修改默认路劲。\n\n```\n>Enter passphrase (empty for no passphrase):<enter a passphrase>\nEnter same passphrase again:<enter passphrase again>\n```\n\n  设置一个密码短语，在每次远程操作之前会要求输入密码短语！闲麻烦可以直接回车，不设置。\n\n成功：\n\n```\nYour identification has been saved in /Users/your_user_directory/.ssh/id_rsa.\nYour public key has been saved in /Users/your_user_directory/.ssh/id_rsa.pub.\nThe key fingerprint is:\n... ...\n```\n\n默认会在路径`~/.ssh`下生成`id_rsa`和`id_rsa.pub`两个文件。\n\n<br>\n\n- 将 SSH key 添加到 GitHub 中\n\n终端输入：\n\n```\ncat ~/.ssh/id_rsa.pub\n```\n\n复制内容\n\n进入GitHub –> Settings –> SSH and GPG keys –> NEW SSH key\n\nTitle 里任意添一个标题，将复制的内容粘贴到 Key 里，点击下方 Add SSH key 绿色按钮即可\n\n<br>\n\n### 部署到 GitHub 上\n\n打开当前文件夹（blog）下的 `_config.yml` 文件，在末尾deploy后面添加几行代码：\n\n```\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repository: git@github.com:yourname/yourname.github.io.git\n  branch: master\n```\n\n注意：冒号后面有空格，yourname 替换成你自己的 GitHub 用户名。（repository 可以在 Github 上复制，但记得选`Clone with SSH` ）\n\n\n\n终端输入：\n\n```\nnpm install hexo-deployer-git --save\nhexo deploy\n```\n\n显示 `INFO  Deploy done: git` 表示完成。\n\n<br>\n\n### 设置独立域名\n\n博客根目录的 `source/` 目录下创建一个名叫 `CNAME` 的文件，注意没有后缀，里面写你的域名。\n\n部署，添加域名解析，就 OK 了\n\n域名解析请参考[“极简”创建 github page 并设置域名](https://www.jianshu.com/p/04ab48de1720)\n\n<br>\n\n### 编辑文章\n\n- 创建文章\n  命令行输入：\n\n```\nhexo new \"new article\"\n```\n\n之后在博客根目录 /source/_posts/ 文件夹下面，多了一个 new-article.md 文件。\n\n打开之后我们会看到：\n\n```\ntitle: new article\ndate: 2018-12-10 20:10:33\ntags:\n```\n\n> `title` 是当前文档名，也是将来在网页中显示的文章标题。\n>\n> `date` 是我们新建文档的时间。\n>\n> `tags` 是文档的标签，我们用次来为文章加标签。\n\n为文章加标签：\n\n```\n---\ntitle: new article\ndate: 2018-12-10 20:10:33\ntags:\n- tag1\n- tag2\n- tag3\n---\n```\n\n上面的文章贴上了 `tag1`、`tag2`、`tag3` 标签\n\n<br>\n\n- 删除文章\n\n博客根目录 `/source/_posts/` 文件夹下，删除对应文章的 `.md` 文件\n\n`_posts/` 文件夹不能为空，否则网页会报错 `Cannot GET /`\n\n<br>\n\n### 更新博客\n\n- 本地预览\n\n```\nhexo clean\nhexo g\nhexo s\n```\n\n打开 http://localhost:4000/ 预览效果\n\n\n\n- 部署到 GitHub\n\n```\nhexo clean\nhexo g\nhexo d\n```\n\n<br>\n\n\n\n------\n\n鸣谢：\n[Hexo + GitHub (Coding) Pages 搭建博客](https://github.com/HarleyWang93/blog/issues/1)\n[使用 Hexo 为自己在 Github 上建一个静态 Blog 站点](http://lixiaolai.com/2016/06/22/makecs-build-a-blog-with-hexo-on-github/)\n\n2018-12-11\n\n","tags":["blog","hexo"]},{"title":"三种建博方式","url":"/2018/12/11/三种建博方式/","content":"\n\n\n<br>\n\n\n\n1.博客平台\n\n简书、CSDN、知乎专栏等\n\n简单，可控性低\n\n申请个账号就可以写，其他的都不用管\n\n限制多，界面、排版、有没有广告自己说了不算，什么能发什么不能发，自己也说了不算\n\n<br>\n\n2.自己建站\n\n复杂度高，可控性高\n\n啥都能控制，但啥都靠自己，服务器、域名、写代码、部署，网站排名、防护等等\n\n<br>\n\n3.Github Page\n\n复杂度中，可控性比较高\n\n有控制权，但不用事事自己管。除了网页是静态的，几乎没有别的限制。界面、排版自己控制，需要点编程基础。服务器，网站防护等不用自己管。\n\n<br>\n\n------\n\n<br>\n\n[我的博客](https://v1coder.com/)\n\n[我的简书](https://www.jianshu.com/u/7cb04d09491e)","tags":["blog"]},{"title":"Hello World","url":"/2018/12/10/Hello-World/"}]